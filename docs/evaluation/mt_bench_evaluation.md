---
sidebar_position: 2
---

# MT-Bench Evaluation

MT-Bench is a set of challenging multi-turn open-ended questions designed to assess the quality of large language models. To automate the evaluation process, strong LLMs like GPT-4 are prompted to act as judges and evaluate the model's performance.

To set up and conduct an MT-Bench evaluation for FastChat models, follow the instructions in the [MT-Bench Evaluation Guide](/docs/evaluation/mt_bench_evaluation.md).
