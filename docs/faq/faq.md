---
sidebar_position: 1
---

# Frequently Asked Questions

This section addresses common questions and concerns related to FastChat.

Q: What models does FastChat support?

A: FastChat supports a wide range of models, including LLama 2, Vicuna, Alpaca, Baize, ChatGLM, Dolly, Falcon, FastChat-T5, GPT4ALL, Guanaco, MTP, OpenAssistant, OpenChat, RedPajama, StableLM, WizardLM, xDAN-AI, and more. See the [Supported Models](/docs/inference/cli_inference.md) section for a complete list.

Q: How do I add new models to FastChat?

A: To add a new model to FastChat, follow the instructions in the [Supported Models Documentation](/docs/inference/cli_inference.md) to create and implement the necessary additions to the FastChat system.

For more frequently asked questions, visit the [FAQ section](/docs/faq/faq.md).
