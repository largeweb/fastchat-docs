---
sidebar_position: 3
---

# ExLlama V2 for Faster Inference

ExLlama V2 is an optimized technique for LLM inference. It can significantly speed up the inference process without sacrificing the model's performance.

For more information about ExLlama V2 and how to use it with FastChat, refer to [ExLlama V2 official documentation](https://exllama-project.io/docs/v2).
