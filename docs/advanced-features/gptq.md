---
sidebar_position: 4
---

# GPTQ for Efficient Inference

GPTQ is a model compression method that uses 4-bit quantization for more efficient LLM inference. It improves the memory and computational efficiency without considerable loss in model performance.

For more information on GPTQ and its integration with FastChat, refer to [GPTQ for LLM official documentation](https://gptq-project.io/docs).
